{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Objective**\n",
        "\n",
        "The aim of this exercise is to understand the RL dynamics of training an LLM model.\n",
        "\n",
        "**Rules for this assignment**\n",
        "\n",
        "You are allowed to use any resources available on the internet to complete this assignment. The only restriction is that you should work on your own without discussing with any other people."
      ],
      "metadata": {
        "id": "yyjvxItJnS3O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1 -- Background**\n",
        "\n",
        "**RL Algorithm**\n",
        "\n",
        "For future stages in the take home it will be helpful to be familiar with basic RL algorithms. You should feel free to experiment with any algorithm you think is best, for now, make sure you know about several classical RL algorithms: Policy gradient, Proximal Policy Optimization, Group Relative Policy Optimization, Expert Imitation etc. **Be ready to discuss your understanding with the interviewer**."
      ],
      "metadata": {
        "id": "h7HxiT8dxiFB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2**\n",
        "\n",
        "**RL Environment**\n",
        "\n",
        "Think of a task / dataset that would be a good fit for RL training an LLM. We want to focus on tasks that improve the reasoning capabilities of LLMs. The broad category of tasks that are relevant to this exercise include STEM (e.g., math datasets) and coding (e.g., Python), or any taks you think may be important for reasoining.\n",
        "\n",
        "Here, you should **create a small dataset that is conducive to training** the model. Feel free to use any resource possible.\n",
        "\n",
        "*Hint: When coming up with a task, think about what the reward structure for that task will be?*"
      ],
      "metadata": {
        "id": "5gsYqzKiyq-X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V8jESXIhHv04"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3**\n",
        "\n",
        "**Sampling**\n",
        "\n",
        "Choose a base LLM model. In colab, you have access to a T4 GPU (for free tier) and A100 (for paid tier). Depending on the type of GPU you have access to, think about some open-source base LLM models that would be good to experiment with given your setup.\n",
        "\n",
        "Here, you should **download the model and run inference on some of the prompts you collected in the dataset in Step 2**.\n",
        "\n",
        "*Hint: We do not care about absolute performance, we only care about studying the RL dynamics.*"
      ],
      "metadata": {
        "id": "QCQQ79Op0Iu9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jmKzGIwC0yNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4**\n",
        "\n",
        "**Eval Metrics**\n",
        "\n",
        "We would like to see the model learning new things! For the dataset in step 2 and the model in step 3, before running actual RL training, how do you know if the tasks are appropriate for an RL experiment? Think about evaluation metrics that can be informative, and **run your evaluation on the model**. Be ready to discuss your evaluation results and your understanding.\n",
        "\n",
        "*Hint: A good RL experiment should ensure:*\n",
        "*   The model can learn during RL.\n",
        "*   The learning process can continue for a reasonable long time for us to study dynamics."
      ],
      "metadata": {
        "id": "VgsS6SJA49VP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vTSwT1xl6vEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5**\n",
        "\n",
        "**Training** Now, implement RL algorithm that trains your model from Step 3 on the dataset in Step 2.\n",
        "\n",
        "*Hint: Have a held out validation set so that we can monitor both training and validation reward as the RL training progresses.*"
      ],
      "metadata": {
        "id": "0a_bWEot0xi1"
      }
    }
  ]
}