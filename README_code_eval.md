# EinsteinToolkit Code Generation and Evaluation

This script integrates all the previously tested components to create a complete pipeline for generating and evaluating EinsteinToolkit code.

## Overview

The script performs the following steps:
1. Loads the HuggingFace dataset `xinshuo/ET_code_with_context`
2. Filters examples to only include thorns that have tests (from `thorn_to_tests.txt`)
3. Generates code using the remote API with EinsteinToolkit-specific prompts
4. Deploys generated code to a Docker container
5. Builds the code and runs corresponding tests
6. Scores the results based on build success and test outcomes
7. Generates detailed reports and saves all results

## Prerequisites

1. **Docker**: Make sure Docker is installed and the EinsteinToolkit image is available:
   ```bash
   docker pull rynge/einsteintoolkit
   ```

2. **Python Dependencies**: Install required packages:
   ```bash
   pip install datasets transformers torch openai
   ```

3. **API Key**: Set your API key as an environment variable:
   ```bash
   export SEED_LLM_API_KEY="your_api_key_here"
   ```

## Usage

### Basic Usage

Run with default settings (3 examples):
```bash
python3 einstein_toolkit_code_eval.py
```

### Advanced Usage

```bash
# Process 10 examples
python3 einstein_toolkit_code_eval.py --max-examples 10

# Specify API key directly
python3 einstein_toolkit_code_eval.py --api-key "your_key" --max-examples 5

# Specify output directory
python3 einstein_toolkit_code_eval.py --output-dir "results" --max-examples 2
```

### Mock Mode (for testing without API)

```bash
MOCK_API=1 python3 einstein_toolkit_code_eval.py --max-examples 1
```

## Output Files

The script generates several output files in the specified directory (`scratch` by default):

1. **Generated Code Files**: `generated_N_filename.c` - The actual code generated by the API
2. **Results JSON**: `evaluation_results_YYYYMMDD_HHMMSS.json` - Detailed results for each example
3. **Build Logs**: Available in the Docker container under `/opt/Cactus/log_files/`

## Results Structure

Each result in the JSON file contains:
- **example_index**: Original index in the dataset
- **thorn_name**: Name of the EinsteinToolkit thorn
- **src_filename**: Target source file name
- **generated_code_file**: Path to the generated code file
- **build_result**: Build success/failure and error details
- **test_result**: Test execution results and statistics
- **score_result**: Overall score (0-100) with breakdown and letter grade

## Scoring System

The scoring system evaluates generated code based on:
- **Build Success** (40%): Whether the code compiles successfully
- **Test Success Rate** (50%): Percentage of tests that pass
- **Error Penalties** (10%): Deductions for various types of errors

Grades:
- A: 90-100 (Excellent)
- B: 80-89 (Good)
- C: 70-79 (Satisfactory)
- D: 60-69 (Needs Improvement)
- F: 0-59 (Failed)

## Example Output

```
EinsteinToolkit Code Generation and Evaluation
============================================================
Max examples: 3
Output directory: scratch
API available: Yes

Loading HuggingFace dataset...
Dataset loaded successfully. Total examples: 3068
Valid thorns loaded: 66
Valid examples after filtering: 3

Starting Docker container: ET_code_eval_container
...
Processing example 1/3: CactusExamples/Poisson - uniform_charge.c
Generated code saved to: scratch/generated_1_uniform_charge.c
Build SUCCEEDED
Tests: 1/1 passed
Overall Score: 85.0/100 (Grade: B)
...

EVALUATION COMPLETE
================================================================================
Processed 3 examples
Results saved to: scratch/evaluation_results_20250924_165022.json
Average score: 72.3/100
Grade distribution: {'A': 1, 'B': 1, 'C': 1}
```

## Troubleshooting

### Docker Issues
- Make sure Docker is running: `docker --version`
- Pull the EinsteinToolkit image: `docker pull rynge/einsteintoolkit`
- Check for port conflicts or insufficient resources

### API Issues
- Verify your API key is correct
- Check network connectivity
- Use mock mode for testing: `MOCK_API=1`

### Build Failures
- This is expected for mock generated code
- Real API-generated code should have higher success rates
- Check the build logs in the results JSON for specific error details

## Files Structure

```
ET_develop/
├── einstein_toolkit_code_eval.py    # Main integrated script
├── api_usage.py                      # API examples (reference)
├── run_all_tests.sh                  # Test runner script
├── thorn_to_tests.txt               # Thorn-to-tests mapping
├── validate_thorns.py               # Validation utilities
├── scratch/                         # Output directory
│   ├── generated_*.c               # Generated code files
│   ├── evaluation_results_*.json   # Results
│   └── test_*.py                   # Development test files
└── README_code_eval.md             # This file
```

## Development

For development and debugging, see the test files in the `scratch/` directory:
- `test_dataset_load.py` - Test dataset loading
- `test_api_function.py` - Test API interaction
- `test_docker_management.py` - Test Docker operations
- `test_build_functions.py` - Test build and test execution
- `test_scoring_system.py` - Test scoring calculations

## Notes

- The script requires significant computational resources due to Docker and compilation
- Each run can take several minutes to hours depending on the number of examples
- The Docker container is automatically cleaned up after each run
- Generated code and results are preserved for manual inspection
